{
  "metadata" : {
    "name" : "AggregateAndSaveToCassandra",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "org.bdgenomics.adam % adam-core % 0.15.0", "- org.apache.hadoop % hadoop-client %   _", "- org.apache.spark  % spark-core    %   _", "- org.scala-lang    %     _         %   _", "- org.scoverage     %     _         %   _", "- org.apache.spark  % sql           %   _", "com.datastax.spark % spark-cassandra-connector_2.10 % 1.2.3" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.cores" : "2",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.cores.max" : "2",
      "spark.eventLog.dir" : "logs/spark",
      "spark.cassandra.connection.host" : "127.0.0.1",
      "spark.eventLog.enabled" : "true",
      "spark.executor.memory" : "512m",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryoserializer.buffer.mb" : "4",
      "spark.kryo.referenceTracking" : "true"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Import sparkSQL classes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nval sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@65cc8946\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Data directory"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataRoot = \"/root/pipeline/datasets\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataRoot: String = /root/pipeline/datasets\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/root/pipeline/datasets"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Parquet formated FlatGenotypes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataFile = s\"$dataRoot/flat-genotypes13\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataFile: String = /root/pipeline/datasets/flat-genotypes13\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/root/pipeline/datasets/flat-genotypes13"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Load the data: \nWe read the parquet files..."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gdf = sqlContext.read.parquet(dataFile)\ngdf.cache()\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "gdf: org.apache.spark.sql.DataFrame = [population: string, sampleId: string, chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gdf.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: Long = 2342844\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "2342844"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We do aggregation computations:\n\nCount reference and alternate alleles for each population"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val byPopulation = gdf.groupBy(\"population\", \"chromosome\", \"start\", \"ref\", \"alt\").agg(sum(gdf(\"refCnt\")) as \"refCnt\", sum(gdf(\"altCnt\")) as \"altCnt\")\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "byPopulation: org.apache.spark.sql.DataFrame = [population: string, chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "And we count of the complete dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val allPopulations = gdf.groupBy(\"chromosome\", \"start\", \"ref\", \"alt\")\n                                 .agg(sum(gdf(\"refCnt\")) as \"refCnt\", sum(gdf(\"altCnt\")) as \"altCnt\")\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "allPopulations: org.apache.spark.sql.DataFrame = [chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "allPopulations.show",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+--------+---+---+------+------+\n|chromosome|   start|ref|alt|refCnt|altCnt|\n+----------+--------+---+---+------+------+\n|        22|17699864|  G|  A|  1904|   162|\n|        22|16963544|  A|  G|   924|  1142|\n|        22|17143841|  T|  G|  1547|   519|\n|        22|17586582|  C|  G|   900|  1166|\n|        22|17293393|  G|  A|  2057|     9|\n|        22|17815434|  C|  T|  1897|   169|\n|        22|17518657|  A|  G|  1993|    73|\n|        22|17385485|  C|  A|  2065|     1|\n|        22|17625686|  T|  C|  2046|    20|\n|        22|17008979|  C|  T|  1974|    92|\n|        22|17804722|  G|  A|  1998|    68|\n|        22|17052802|  A|  G|  2063|     3|\n|        22|17783922|  G|  A|  1129|   937|\n|        22|17445922|  G|  A|  2063|     3|\n|        22|17334473|  T|  A|  2051|    15|\n|        22|17047160|  T|  C|   102|  1964|\n|        22|17684251|  G|  A|  2053|    13|\n|        22|17313543|  C|  A|  2063|     3|\n|        22|17712851|  G|  A|  2066|     0|\n|        22|17990492|  C|  T|  2065|     1|\n+----------+--------+---+---+------+------+\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We defined a structure to hold count data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "object serCtx extends java.io.Serializable {\n  case class PopAlleleCount(\n  population: String,\n  chromosome: String,\n  start: Long,\n  ref: String,\n  alt: String,\n  refcnt: Long,\n  altcnt: Long\n    ) extends java.io.Serializable\n}\nimport serCtx._\n    ",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "defined module serCtx\nimport serCtx._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 29
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### And covert Dataframes aggregates to RDDs"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val countsByPop = byPopulation.rdd.collect {\n   case Row(pop: String, chr: String, start: Long, ref: String, alt: String, refcnt: Long, altcnt: Long) => \n          PopAlleleCount(pop, chr, start, ref, alt, refcnt, altcnt)\n   }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "countsByPop: org.apache.spark.rdd.RDD[serCtx.PopAlleleCount] = MapPartitionsRDD[36] at collect at <console>:83\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[36] at collect at &lt;console&gt;:83"
      },
      "output_type" : "execute_result",
      "execution_count" : 30
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val countAll = allPopulations.rdd.collect {\n    case Row(chr: String, start: Long, ref: String, alt: String, refcnt: Long, altcnt: Long) => \n          PopAlleleCount(\"ALL\", chr, start, ref, alt, refcnt, altcnt)\n   }",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "countAll: org.apache.spark.rdd.RDD[serCtx.PopAlleleCount] = MapPartitionsRDD[38] at collect at <console>:83\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[38] at collect at &lt;console&gt;:83"
      },
      "output_type" : "execute_result",
      "execution_count" : 31
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Now we can save countsByPop and countAll to Cassandra for querying"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "A schema could look like...\n\n<pre>\nCREATE TABLE pop_allele_count (\n  population: text,\n  chromosome: text,\n  start: long,\n  ref: text,\n  alt: text,\n  refCnt: long,\n  altCnt: long,\n  PRIMARY KEY (population, chromosome, start) \n  );\n\nSELECT * FROM pop_allele_count WHERE population = 'ALL' AND chromosome = '22' \n           AND start >= 16500000 AND start < 16750000;\n</pre>"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countsByPop.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res20: Long = 15876\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "15876"
      },
      "output_type" : "execute_result",
      "execution_count" : 32
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countAll.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res21: Long = 1134\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "1134"
      },
      "output_type" : "execute_result",
      "execution_count" : 33
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 40
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val cc = CassandraConnector(sparkContext.getConf)",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "cc: com.datastax.spark.connector.cql.CassandraConnector = com.datastax.spark.connector.cql.CassandraConnector@2a422d61\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "com.datastax.spark.connector.cql.CassandraConnector@2a422d61"
      },
      "output_type" : "execute_result",
      "execution_count" : 41
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"CREATE KEYSPACE IF NOT EXISTS pipeline WITH REPLICATION = { 'class':'SimpleStrategy', 'replication_factor':1}\")}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res27: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ResultSet[ exhausted: true, Columns[]]"
      },
      "output_type" : "execute_result",
      "execution_count" : 42
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"DROP TABLE IF EXISTS pipeline.pop_allele_count\")}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res28: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ResultSet[ exhausted: true, Columns[]]"
      },
      "output_type" : "execute_result",
      "execution_count" : 43
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"CREATE TABLE pipeline.pop_allele_count (population text,chromosome text, start double, ref text, alt text, refCnt double, altCnt double, PRIMARY KEY (population, chromosome, start) );\")}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res29: com.datastax.driver.core.ResultSet = ResultSet[ exhausted: true, Columns[]]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "ResultSet[ exhausted: true, Columns[]]"
      },
      "output_type" : "execute_result",
      "execution_count" : 44
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countsByPop.saveToCassandra(\"pipeline\",\"pop_allele_count\", SomeColumns(\"population\",\"chromosome\",\"start\",\"ref\",\"alt\",\"refcnt\",\"altcnt\")) ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "val",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}