{
  "metadata" : {
    "name" : "AggregateAndSaveToCassandra",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "org.bdgenomics.adam % adam-core % 0.15.0", "- org.apache.hadoop % hadoop-client %   _", "- org.apache.spark  % spark-core    %   _", "- org.scala-lang    %     _         %   _", "- org.scoverage     %     _         %   _", "- org.apache.spark  % sql           %   _", "com.datastax.spark % spark-cassandra-connector_2.10 % 1.4.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.cores" : "2",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.cores.max" : "2",
      "spark.eventLog.dir" : "logs/spark",
      "spark.cassandra.connection.host" : "172.17.0.7",
      "spark.eventLog.enabled" : "true",
      "spark.executor.memory" : "512m",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryoserializer.buffer.mb" : "4",
      "spark.kryo.referenceTracking" : "true",
      "spark.cassandra.output.batch.grouping.key" : "None",
      "spark.cassandra.output.batch.size.rows" : "10",
      "spark.cassandra.output.batch.size.bytes" : "2048",
      "spark.sql.shuffle.partitions" : "16"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Store stats in Cassandra"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "In this notebook, we'll take the data from the [Adam to Dataframe](/notebooks/pipeline/AdamToDataframe.snb) notebook and compute some statistics on the saved data.\n\nMany very helpful stats can be computed on such dataset, here we'll focus on [**allele frequencies**](https://en.wikipedia.org/wiki/Allele_frequency)."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Create SQL context to use DataFrame"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nval sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## The source"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We'll read the dataset created in the previous notebook: the Parquet formated FlatGenotypes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataFile = s\"/tmp/flat-genotypes13\"",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Loading the dataset \n\nUsing the data source api from Spark, we can read the parquet file and already be aware of the structure thanks to the schema stored in it. \n\nWe get back a simple DataFrame with the right structure."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gdf = sqlContext.read.parquet(dataFile)\ngdf.cache()\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gdf.count",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Aggregations (sums) per population\n\nCount reference and alternate alleles for each population"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val byPopulation = gdf.groupBy(\"population\", \"chromosome\", \"start\", \"ref\", \"alt\")\n                      .agg(\n                        sum($\"refCnt\") as \"refCnt\", \n                        sum($\"altCnt\") as \"altCnt\"\n                      )\n()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## Aggregations (sums) on the whole dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val allPopulations = gdf.groupBy(\"chromosome\", \"start\", \"ref\", \"alt\")\n                        .agg(\n                          sum($\"refCnt\") as \"refCnt\", \n                          sum($\"altCnt\") as \"altCnt\"\n                        )\n()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Let's take a look at the result"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "allPopulations",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Now we can save countsByPop and countAll RDDs to Cassandra for querying"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### A schema could look like...\n\n```sql\nCREATE TABLE pop_allele_count (\n  population text,\n  chromosome text,\n  start double,\n  ref text,\n  alt text,\n  refcnt double,\n  altcnt double,\n  PRIMARY KEY (population, chromosome, start) \n);\n```\n\nThen we can query like this...\n\n```sql\nSELECT * \nFROM pop_allele_count \nWHERE \n  population = 'ALL' AND \n  chromosome = '22'  AND \n  start >= 16500000  AND \n  start < 16750000;\n```"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Import libraries required for Spark to access Cassandra"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Create a Cassandra Context using the current Spark Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val cc = CassandraConnector(sparkContext.getConf)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Create a Cassandra keyspace to hold our table(s)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => \n  session.execute(s\"\"\"\n      CREATE KEYSPACE IF NOT EXISTS pipeline \n        WITH REPLICATION = { 'class':'SimpleStrategy', 'replication_factor':1}\n    \"\"\".stripMargin\n  )\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Drop any existing table and create a new one"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => \n  session.execute(s\"\"\"\n      DROP TABLE IF EXISTS pipeline.pop_allele_count\n    \"\"\"\n  )}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"\"\"\n      CREATE TABLE pipeline.pop_allele_count (\n        population text,\n        chromosome text,\n        start double,\n        ref text,\n        alt text,\n        ref_cnt double,\n        alt_cnt double,\n        PRIMARY KEY (population, chromosome, start) \n      );\n    \"\"\"\n  )\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Use the DataFrame and Data Source API to both contents to a Cassandra table"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "> Note that the names in the DataFrame and the Cassandra tables aren't exactly mathching!\n> \n> Hence, we rename them before calling the save method."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "byPopulation\n    .withColumnRenamed(\"refCnt\", \"ref_cnt\")\n    .withColumnRenamed(\"altCnt\", \"alt_cnt\")\n    .write.format(\"org.apache.spark.sql.cassandra\")\n    .mode(org.apache.spark.sql.SaveMode.Append)\n    .options(Map(\"keyspace\" -> \"pipeline\", \"table\" -> \"pop_allele_count\"))\n    .save()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "> Note that the `allPopulation` DataFrame hasn't the `population` field since it's for the whole dataset\n> \n> Hence, in order to reuse the same table, we'll add the `population` column always containing the `\"ALL\"` value."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "allPopulations\n    .withColumn(\"population\", lit(\"ALL\"))\n    .withColumnRenamed(\"refCnt\", \"ref_cnt\")\n    .withColumnRenamed(\"altCnt\", \"alt_cnt\")\n    .write.format(\"org.apache.spark.sql.cassandra\")\n    .mode(org.apache.spark.sql.SaveMode.Append)\n    .options(Map(\"keyspace\" -> \"pipeline\", \"table\" -> \"pop_allele_count\"))\n    .save()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Check results\n\nAt this point you should be able to query Cassandra to check how many records have been written:\n\n```\n$ cqlsh $IP_eth0\n\nConnected to Test Cluster at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 2.2.0 | CQL spec 3.3.0 | Native protocol v4]\nUse HELP for help.\ncqlsh:pipeline> select count(*) from pipeline.pop_allele_count ;\n\n count\n-------\n 17010\n\ncqlsh:pipeline> SELECT * FROM pop_allele_count \nWHERE population = 'ALL' \nAND chromosome = '22' \nAND start >= 16500000 \nAND start < 16750000;\n\n population | chromosome | start      | alt | altcnt | ref | refcnt\n------------+------------+------------+-----+--------+-----+--------\n        ALL |         22 | 1.6505e+07 |   T |     57 |   C |   2009\n        ALL |         22 | 1.6508e+07 |   A |      1 |   C |   2065\n        ALL |         22 |  1.652e+07 |   G |    146 |   C |   1920\n        ALL |         22 | 1.6524e+07 |   T |     19 |   G |   2047\n        ALL |         22 | 1.6535e+07 |   T |     13 |   G |   2053\n        ALL |         22 | 1.6543e+07 |   C |     35 |   T |   2031\n        ALL |         22 | 1.6554e+07 |   T |     12 |   C |   2054\n```"
  } ],
  "nbformat" : 4
}