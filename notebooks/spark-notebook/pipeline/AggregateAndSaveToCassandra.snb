{
  "metadata" : {
    "name" : "AggregateAndSaveToCassandra",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "org.bdgenomics.adam % adam-core % 0.15.0", "- org.apache.hadoop % hadoop-client %   _", "- org.apache.spark  % spark-core    %   _", "- org.scala-lang    %     _         %   _", "- org.scoverage     %     _         %   _", "- org.apache.spark  % sql           %   _", "com.datastax.spark % spark-cassandra-connector_2.10 % 1.4.0" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.cores" : "2",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.cores.max" : "2",
      "spark.eventLog.dir" : "logs/spark",
      "spark.cassandra.connection.host" : "127.0.0.1",
      "spark.eventLog.enabled" : "true",
      "spark.executor.memory" : "512m",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryoserializer.buffer.mb" : "4",
      "spark.kryo.referenceTracking" : "true",
      "spark.cassandra.output.batch.grouping.key" : "None",
      "spark.cassandra.output.batch.size.rows" : "10",
      "spark.cassandra.output.batch.size.bytes" : "2048",
      "spark.sql.shuffle.partitions" : "16"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Import sparkSQL classes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.rdd.RDD\nval sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.rdd.RDD\nsqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@13ff7337\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.Row\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Data directory"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataRoot = \"/root/pipeline/datasets\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataRoot: String = /root/pipeline/datasets\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/root/pipeline/datasets"
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Parquet formated FlatGenotypes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataFile = s\"$dataRoot/flat-genotypes13\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "dataFile: String = /root/pipeline/datasets/flat-genotypes13\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/root/pipeline/datasets/flat-genotypes13"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Load the data: \nWe read the parquet files..."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gdf = sqlContext.read.parquet(dataFile)\ngdf.cache()\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "gdf: org.apache.spark.sql.DataFrame = [population: string, sampleId: string, chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gdf.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res3: Long = 2342844\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "2342844"
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We do aggregation computations:\n\nCount reference and alternate alleles for each population"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val byPopulation = gdf.groupBy(\"population\", \"chromosome\", \"start\", \"ref\", \"alt\").agg(sum(gdf(\"refCnt\")) as \"refCnt\", sum(gdf(\"altCnt\")) as \"altCnt\")\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "byPopulation: org.apache.spark.sql.DataFrame = [population: string, chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 6
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "And we count of the complete dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val allPopulations = gdf.groupBy(\"chromosome\", \"start\", \"ref\", \"alt\")\n                                 .agg(sum(gdf(\"refCnt\")) as \"refCnt\", sum(gdf(\"altCnt\")) as \"altCnt\")\n()",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "allPopulations: org.apache.spark.sql.DataFrame = [chromosome: string, start: bigint, ref: string, alt: string, refCnt: bigint, altCnt: bigint]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 7
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "allPopulations.show",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "+----------+--------+---+---+------+------+\n|chromosome|   start|ref|alt|refCnt|altCnt|\n+----------+--------+---+---+------+------+\n|        22|17699864|  G|  A|  1904|   162|\n|        22|16963544|  A|  G|   924|  1142|\n|        22|17143841|  T|  G|  1547|   519|\n|        22|17586582|  C|  G|   900|  1166|\n|        22|17293393|  G|  A|  2057|     9|\n|        22|17815434|  C|  T|  1897|   169|\n|        22|17518657|  A|  G|  1993|    73|\n|        22|17385485|  C|  A|  2065|     1|\n|        22|17625686|  T|  C|  2046|    20|\n|        22|17008979|  C|  T|  1974|    92|\n|        22|17804722|  G|  A|  1998|    68|\n|        22|17052802|  A|  G|  2063|     3|\n|        22|17783922|  G|  A|  1129|   937|\n|        22|17445922|  G|  A|  2063|     3|\n|        22|17334473|  T|  A|  2051|    15|\n|        22|17047160|  T|  C|   102|  1964|\n|        22|17684251|  G|  A|  2053|    13|\n|        22|17313543|  C|  A|  2063|     3|\n|        22|17712851|  G|  A|  2066|     0|\n|        22|17990492|  C|  T|  2065|     1|\n+----------+--------+---+---+------+------+\n\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 8
    } ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We defined a structure to hold count data"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "object serCtx extends java.io.Serializable {\n  case class PopAlleleCount(\n  population: String,\n  chromosome: String,\n  start: Long,\n  ref: String,\n  alt: String,\n  refcnt: Long,\n  altcnt: Long\n    ) extends java.io.Serializable\n}\nimport serCtx._\n    ",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Convert Dataframe aggregates to RDDs"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val countsByPop = byPopulation.rdd.collect {\n   case Row(pop: String, chr: String, start: Long, ref: String, alt: String, refcnt: Long, altcnt: Long) => \n          PopAlleleCount(pop, chr, start, ref, alt, refcnt, altcnt)\n   }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val countAll = allPopulations.rdd.collect {\n    case Row(chr: String, start: Long, ref: String, alt: String, refcnt: Long, altcnt: Long) => \n          PopAlleleCount(\"ALL\", chr, start, ref, alt, refcnt, altcnt)\n   }",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Now we can save countsByPop and countAll RDDs to Cassandra for querying"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### A schema could look like...\n\n<pre>\nCREATE TABLE pop_allele_count (\n  population text,\n  chromosome text,\n  start double,\n  ref text,\n  alt text,\n  refcnt double,\n  altcnt double,\n  PRIMARY KEY (population, chromosome, start) \n  );\n</pre>\n\nThen we can query like this...\n\n<pre>\nSELECT * FROM pop_allele_count WHERE population = 'ALL' AND chromosome = '22' \n           AND start >= 16500000 AND start < 16750000;\n</pre>"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Check the number of records in the RDDs"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countsByPop.count",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "res7: Long = 15876\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "15876"
      },
      "output_type" : "execute_result",
      "execution_count" : 12
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countAll.count",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Import libraries required for Spark to access Cassandra"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import com.datastax.spark.connector.cql.CassandraConnector\nimport com.datastax.spark.connector._",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Create a Cassandra Context using the current Spark Context"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val cc = CassandraConnector(sparkContext.getConf)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Create a Cassandra keyspace to hold our table(s)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"CREATE KEYSPACE IF NOT EXISTS pipeline WITH REPLICATION = { 'class':'SimpleStrategy', 'replication_factor':1}\")}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Drop any existing table and create a new one"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"DROP TABLE IF EXISTS pipeline.pop_allele_count\")}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "cc.withSessionDo { session => session.execute(s\"CREATE TABLE pipeline.pop_allele_count (population text,chromosome text, start double, ref text, alt text, refCnt double, altCnt double, PRIMARY KEY (population, chromosome, start) );\")}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Use the saveToCassandra RDD method to save both the RDD contents to a Cassandra table"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countsByPop.saveToCassandra(\"pipeline\",\"pop_allele_count\", SomeColumns(\"population\",\"chromosome\",\"start\",\"ref\",\"alt\",\"refcnt\",\"altcnt\")) ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "countAll.saveToCassandra(\"pipeline\",\"pop_allele_count\", SomeColumns(\"population\",\"chromosome\",\"start\",\"ref\",\"alt\",\"refcnt\",\"altcnt\")) ",
    "outputs" : [ ]

  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Check results\n\nAt this point you should be able to query Cassandra to check how many records have been written:\n\n<pre>\n$ cqlsh\n\nConnected to Test Cluster at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 2.2.0 | CQL spec 3.3.0 | Native protocol v4]\nUse HELP for help.\ncqlsh:pipeline> select count(*) from pipeline.pop_allele_count ;\n\n count\n-------\n 17010\n\ncqlsh:pipeline> SELECT * FROM pop_allele_count \nWHERE population = 'ALL' \nAND chromosome = '22' \nAND start >= 16500000 \nAND start < 16750000;\n\n population | chromosome | start      | alt | altcnt | ref | refcnt\n------------+------------+------------+-----+--------+-----+--------\n        ALL |         22 | 1.6505e+07 |   T |     57 |   C |   2009\n        ALL |         22 | 1.6508e+07 |   A |      1 |   C |   2065\n        ALL |         22 |  1.652e+07 |   G |    146 |   C |   1920\n        ALL |         22 | 1.6524e+07 |   T |     19 |   G |   2047\n        ALL |         22 | 1.6535e+07 |   T |     13 |   G |   2053\n        ALL |         22 | 1.6543e+07 |   C |     35 |   T |   2031\n        ALL |         22 | 1.6554e+07 |   T |     12 |   C |   2054\n</pre>"
  } ],
  "nbformat" : 4
}