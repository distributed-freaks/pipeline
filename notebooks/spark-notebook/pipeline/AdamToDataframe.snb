{
  "metadata" : {
    "name" : "Adam to Dataframe",
    "user_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp" : "1970-01-01T00:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : "/root/.ivy2",
    "customRepos" : null,
    "customDeps" : [ "org.bdgenomics.adam % adam-core % 0.15.0", "- org.apache.hadoop % hadoop-client %   _", "- org.apache.spark  % spark-core    %   _", "- org.scala-lang    %     _         %   _", "- org.scoverage     %     _         %   _" ],
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : {
      "spark.executor.cores" : "2",
      "spark.master" : "spark://127.0.0.1:7077",
      "spark.cores.max" : "2",
      "spark.eventLog.dir" : "logs/spark",
      "spark.cassandra.connection.host" : "<cassandra-host>",
      "spark.eventLog.enabled" : "true",
      "spark.executor.memory" : "512m",
      "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
      "spark.kryo.registrator" : "org.bdgenomics.adam.serialization.ADAMKryoRegistrator",
      "spark.kryoserializer.buffer.mb" : "4",
      "spark.kryo.referenceTracking" : "true",
      "spark.sql.shuffle.partitions" : "16"
    }
  },
  "cells" : [ {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "# Prepare the data in parquet"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "In this notebook, we'll preprocess a bunch data into a form that can be used in further analyses. So we'll do these things:\n1. take genomics data ([**genotypes**](https://en.wikipedia.org/wiki/Genotype)) for several samples/persons\n2. clean the data (removing rare cases)\n3. associating the samples with their population \n4. remove genotypes for which we don't know the origin\n5. simplify the genotypes and encode their value (allele) into a grid space\n6. save the resulting dataset in a new parquet format using a DataFrame"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "## ADAM"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "The source data is stored locally in this docker instance and has been created using the 1000genomes datasets (stripped, cleaned).\n\nThis dataset has been formated using the ADAM format, which is a Avro/Parquet representation of genomics data.\n\nADAM is an open source project from Berkeley's AMPlab that also adds a genomics API on top of Spark."
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Import some packages from ADAM and Spark"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "ADAM for genomics domain classes\n\nRDD for Spark data interface"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.bdgenomics.formats.avro.Genotype\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.rdd.ADAMContext\n  \nimport org.apache.spark.rdd.RDD",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Data directory"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val dataRoot = \"/root/pipeline/datasets\"",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "Raw dataset: ADAM formated genotypes, chromosome 22 sample (16,000,000 -> 18,000,000)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val adamFile = s\"$dataRoot/chr22-sample13.adam\"",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Load the data: \nWe create an RDD[Genotype]\n\nGenotype is provided by ADAM\n\nRDD is th interface to define transformations and actions on a distributed dataset\n\nadamLoad is a function provided by ADAM (applies on sparkContext through implicit conversion)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gts: RDD[Genotype] = sparkContext.adamLoad(adamFile)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Count the number of genotypes in the dataset"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gts.count",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### What defines a genotype?"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gts.first",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Filter on Single nucleotide Polymorphysims\nIn plain english, we only select the simplest genomic variations: substitution of a base by another"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val snpgts = gts.filter{g => \n               val bases = Set(\"A\",\"T\",\"G\",\"C\")\n               bases.contains(g.getVariant.getReferenceAllele) && \n                                      bases.contains(g.getVariant.getAlternateAllele)\n          }",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We removed some elements"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "snpgts.count",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We import classes and functions to work with Dataframes"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val sqlContext = new org.apache.spark.sql.SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Now we'll read the metadata of the samples (persons)"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We need to point to a file with population data to be joined"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val panelFile = s\"$dataRoot/ALL.panel\"",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "So the below cell is showing the showing the content which is basically associating a sample with:\n* population\n* enclosing population\n* gender"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "output_stream_collapsed" : true,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : ":sh head -n 5 $panelFile",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "#### Now we parse the file and create a Map out of it"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import scala.io.Source\nval panel: Map[String,String] = Source.fromFile(panelFile) // open the file\n                                      .getLines()          // get iterator on lines\n                                      .map{ line =>        // map each line to a tuple (Sample, Population)\n                                        val toks = line.split(\"\\t\").toList // columns are sep. by TAB\n                                        (toks(0), toks(1))                 // 1st Sample, 2nd Population\n                                      }\n                                      .toMap",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Broadcast the Map ( sampleId -> population )\n\nGive every node in the cluster a copy of the Map. So this map won't have to be shipped with **each** task!"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// broadcast the panel \nval bPanel = sparkContext.broadcast(panel)",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### And we filter on the samples with a known population"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "We get rid of the samples not found in the populations we kept in the Map above"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val finalGts = snpgts.filter{g => bPanel.value.contains(g.getSampleId)}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "finalGts.count",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### Some helper functions to format data from ADAM types to simpler schema"
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "These will be used to extract an ID for each variant."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def variantid(g: Genotype): String = {\n  var v = g.getVariant\n  s\"${v.getContig.getContigName}\"\n}",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "This helper is encoding the genotype in a small space where the manathan distance can be used between two genotypes."
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val allelesToPair = (g: Genotype, ga: org.bdgenomics.formats.avro.GenotypeAllele) => ga match {\n  case org.bdgenomics.formats.avro.GenotypeAllele.Ref => (1L, 0L)\n  case org.bdgenomics.formats.avro.GenotypeAllele.Alt => (0L, 1L)\n  case _ => (0L, 0L)\n}",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### We define FlatGenotype, a structure used to store the data we want in a simple table"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "object gCtx extends java.io.Serializable {\n  case class FlatGenotype(\n    population: String,\n    sampleId: String,\n    chromosome: String,\n    start: Long,\n    ref: String,\n    alt: String,\n    refCnt: Long,\n    altCnt: Long\n  )\n}\nimport gCtx._\n    ",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "markdown",
    "source" : "### And now we transform ADAM Genotypes in Flattened structure simpler to count on"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val flatgts = finalGts.flatMap{ g => \n                g.getAlleles.map{ ga => \n                  val al = allelesToPair(g, ga)\n                  FlatGenotype(\n                    bPanel.value.getOrElse(g.getSampleId(), \"\"),\n                    g.getSampleId,\n                    variantid(g),\n                    g.getVariant.getStart,\n                    g.getVariant.getReferenceAllele,\n                    g.getVariant.getAlternateAllele,\n                    al._1,\n                    al._2\n                  )\n                }\n              }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "flatgts.cache()",
    "outputs" : [ ]
  }, {
    "metadata" : { },
    "cell_type" : "markdown",
    "source" : "### We use the dataframe API because saving/reading/aggregation is much easier...\n\nDefault format is parquet, schema included :)"
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val gdf = flatgts.toDF()\n()",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "gdf.write.save(s\"/tmp/flat-genotypes13\")",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}