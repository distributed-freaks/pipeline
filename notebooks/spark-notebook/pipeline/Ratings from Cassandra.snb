{
  "metadata": {
    "name": "Ratings from Cassandra",
    "user_save_timestamp": "1970-01-01T00:00:00.000Z",
    "auto_save_timestamp": "1970-01-01T00:00:00.000Z",
    "language_info": {
      "name": "scala",
      "file_extension": "scala",
      "codemirror_mode": "text/x-scala"
    },
    "trusted": true,
    "customLocalRepo": "/root/.ivy2",
    "customRepos": null,
    "customDeps": [
      "com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3",
      "org.apache.spark:spark-streaming-kafka_2.10:1.4.1",
      "org.apache.spark % spark-graphx_2.10 % 1.4.1",
      "- org.apache.spark % spark-core_2.10 % _",
      "- org.apache.spark % spark-streaming_2.10 % _",
      "- org.apache.hadoop % _ % _"
    ],
    "customImports": null,
    "customSparkConf": {
      "spark.cassandra.connection.host": "<cassandra-host>",
      "spark.master": "spark://127.0.0.1:7077",
      "spark.executor.cores": "2",
      "spark.executor.memory": "512m",
      "spark.cores.max": "2",
      "spark.eventLog.enabled": "true",
      "spark.eventLog.dir": "logs/spark",
      "spark.scheduler.mode": "FAIR"
    }
  },
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Creating the context to use `DataFrame`"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "import org.apache.spark.sql._\nval sqlContext = new SQLContext(sparkContext)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.functions._",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Filling Cassandra"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "<span style=\"font-size:15pt;color:red;\">(**NOT** needed if _feeder_ & _streaming_ are running)</span>"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Creating domain type for `Rating`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "See that it can export to the CSV representation... **AND** in the same time, we drop out the type => isn't _that_ clever?"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "case class Rating(fromUserId: Int, toUserId: Int, rating: Int) {\n  def toCSV=s\"$fromUserId,$rating,$toUserId\"\n}",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Creating the feeder"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "import java.util.Properties\n\nimport scala.concurrent.ExecutionContext.Implicits.global\nimport scala.concurrent.Future\n\nimport org.apache.kafka.clients.producer.{KafkaProducer,ProducerConfig,ProducerRecord}\n\nval props = new Properties()\nprops.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092,localhost:9093\")\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, \"org.apache.kafka.common.serialization.StringSerializer\")\nval producer = new KafkaProducer[String, String](props)\n\n// Guard to stop the producer\nvar stopSending = false\n\n// future that issues a future.\n// a future sends a message after having waited for up to 500ms\ndef sendMsg:Unit = Future {\n  Thread.sleep((scala.util.Random.nextDouble * 500).toLong)\n  producer.send {\n    val msg = Rating(scala.util.Random.nextInt(10000), scala.util.Random.nextInt(10), scala.util.Random.nextInt(10000))\n    new ProducerRecord[String, String](\"ratings\", msg.fromUserId.toString, msg.toCSV)\n  }\n  if (!stopSending) sendMsg\n}\nsendMsg",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Creating the streaming"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Configure spark streaming"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "import org.apache.spark.streaming.Seconds\nimport org.apache.spark.streaming.StreamingContext\n\nval ssc = new StreamingContext(sc, Seconds(2))\nval brokers = \"localhost:9092,localhost:9093\"\nval topics = Set(\"ratings\")\nval kafkaParams = Map[String, String](\"metadata.broker.list\" -> brokers)",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Stream data in Cassandra"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "import org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.Time\nimport kafka.serializer.StringDecoder\n\n// connect (direct) to kafka\nval ratingsStream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](\n  ssc, kafkaParams, topics)\n\n// transform the CSV Strings into Ratings\nval msgs = ratingsStream.foreachRDD { (message: RDD[(String, String)], batchTime: Time) => \n  // convert each RDD from the batch into a DataFrame\n  val df = message.map(_._2.split(\",\"))\n                  .map(rating => \n                       Rating(rating(0).trim.toInt, rating(1).trim.toInt, rating(2).trim.toInt)\n                  ).toDF(\"fromuserid\", \"touserid\", \"rating\")\n\n  // add the batch time to the DataFrame\n  val dfWithBatchTime = df.withColumn(\"batchtime\", org.apache.spark.sql.functions.lit(batchTime.milliseconds))\n\n  // save the DataFrame to Cassandra\n  // Note:  Cassandra has been initialized through spark-env.sh\n  //        Specifically, export SPARK_JAVA_OPTS=-Dspark.cassandra.connection.host=127.0.0.1\n  dfWithBatchTime.write.format(\"org.apache.spark.sql.cassandra\")\n    .mode(SaveMode.Append)\n    .options(Map(\"keyspace\" -> \"pipeline\", \"table\" -> \"real_time_ratings\"))\n    .save()\n}",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Start the streaming job"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "ssc.start()",
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "markdown",
      "source": "# Compute some stats on Cassandra data"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Extra source"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We'll be using an extra dataset that assigns the gender to each rater."
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "val genderDF = sqlContext.read.format(\"json\").load(\"file:/root/pipeline/datasets/dating/gender.json.bz2\")",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Compute ratings' _letter values_ (boxplot) within each gender (grouping)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This will basically:\n\n* load the ratings from Cassandra using Spark\n* compute the rating sum per rated user (`touserid`)\n* create a filtered `DataFrame` for a gender and orders it (for the _quantiles_) \n* compute the count\n* compute the `min` value\n* compute the `max` value\n* compute the quantiles `25`, `50` and `75` (using the count to access the right index in the ordered dataset) "
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "def scopeBox(gender:String, column:String) = new java.io.Serializable {\n  @transient val sqc = sqlContext\n  \n  val ratingsC = sqc\n                  .read\n                  .format(\"org.apache.spark.sql.cassandra\")\n                  .options(Map( \"table\" -> \"real_time_ratings\", \"keyspace\" -> \"pipeline\"))\n                  .load()// This DataFrame will use a spark.cassandra.input.size of 5000\n\n  val ratingColumn = column\n  \n  val sumRatings = ratingsC.groupBy($\"touserid\").agg(Map(ratingColumn -> \"sum\")).withColumnRenamed(\"touserid\", \"id\")\n\n  val gs = sumRatings.join(genderDF, sumRatings(\"id\") === genderDF(\"id\"))\n                          .filter($\"gender\" === gender)\n                          .orderBy(sumRatings(\"SUM(\"+ratingColumn+\")\").asc)\n                          .select(sumRatings(\"SUM(\"+ratingColumn+\")\"))\n                          .map(row => (row.getLong(0)))\n                          .zipWithIndex\n\n  val count = gs.count\n  val min = gs.min._1\n  val max = gs.max._1\n  val qs = gs.filter(x => \n                      x._2 == (count * 0.25).toInt || \n                      x._2 == (count * 0.50).toInt || \n                      x._2 == (count * 0.75).toInt\n                    ).collect().toList\n\n\n  val (q1:Long, _)::(med:Long, _)::(q3:Long, _)::_ = qs\n}",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Creating the BoxPlots (viz)"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "import notebook.front.third.wisp._\nimport com.quantifind.charts.highcharts.Highchart._\nimport com.quantifind.charts.highcharts._\n\nval column = \"batchtime\" // crap thingy here, cassandra seems to mess at write time rating and batchtime\n\nval f =  { \n val b = scopeBox(\"F\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval m =  { \n val b = scopeBox(\"M\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval u =  { \n val b = scopeBox(\"U\", column)\n Data(b.min, b.q1, b.med, b.q3, b.max)\n }\nval series =  Series(\n               Seq(f, m, u), \n               chart=Some(SeriesType.boxplot)\n             )\nval plot = PlotH(Highchart(series, \n                           title=Some(Title(\"Boxplots of rating distributions within Gender\")),\n                           xAxis=Some(Array(Axis(categories=Some(Array(\"Female\", \"Male\", \"Unknown\")))))\n                          )\n                )",
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Cleaning"
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "//ssc.stop(false)",
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": false
      },
      "cell_type": "code",
      "source": "//stopSending=true",
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "input_collapsed": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "",
      "outputs": []
    }
  ],
  "nbformat": 4
}